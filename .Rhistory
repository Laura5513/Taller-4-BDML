lasso<-train(fmla,
data=hog_training,
method = 'glmnet',
trControl = ctrl,
tuneGrid = expand.grid(alpha = 1, #lasso
lambda = seq(0.001,0.02,by = 0.001)),
preProcess = c("center", "scale")
)
head(EN)
print(EN)
coefs_lasso<-coef(lasso$finalModel, lasso$bestTune$lambda)
coef_lasso
summary(ModeloLasso) # Resumen del modelo
ggplot(varImp(ModeloLasso)) # Gráfico de importancia de las variables
ModeloLasso$bestTune
ggplot(varImp(ModeloLasso)) # Gráfico de importancia de las variables
ggplot(vip(ModeloLasso)) # Gráfico de importancia de las variables
vip(ModeloLasso) # Gráfico de importancia de las variables
ggplot(varImp(ModeloLasso), lambda = grid) # Gráfico de importancia de las variables
grid=10^seq(10,-2,length=100)
ModeloLasso<-glmnet(x=X,
y=y,
alpha=1,
lambda=grid)
summary(ModeloLasso) # Resumen del modelo
ggplot(varImp(ModeloLasso), lambda = grid) # Gráfico de importancia de las variables
ggplot(varImp(ModeloLasso, lambda = grid) # Gráfico de importancia de las variables
ggplot(varImp(ModeloLasso, lambda = grid)) # Gráfico de importancia de las variables
varImp(ModeloLasso, lambda = grid)
ggplot(varImp(ModeloLasso, lambda = grid)) # Gráfico de importancia de las variables
summary(ModeloLasso) # Resumen del modelo
varImp(ModeloLasso, lambda = grid)
ggplot(varImp(ModeloLasso, lambda = grid)) # Gráfico de importancia de las variables
ModeloLasso$bestTune
Modelolasso<-glmnet(x=X,
y=y,
alpha=1,
lambda=grid)
Modelolasso<-glmnet(x=X,
y=y,
alpha=1,
lambda=grid)
summary(Modelolasso) # Resumen del modelo
ggplot(varImp(Modelolasso, lambda = grid)) # Gráfico de importancia de las variables
Modelolasso$bestTune
## Gráfico de los coeficientes
#Put coefficients in a data frame, except the intercept
coefs_lasso<-data.frame(t(as.matrix(coef(Modelolasso)))) %>% select(-X.Intercept.)
#add the lambda grid to to data frame
coefs_lasso<- coefs_lasso %>% mutate(lambda=grid)
coef_lasso<-coef(EN$finalModel, EN$bestTune$lambda)
coef_lasso<-coef(Modelolasso$finalModel, Modelolasso$bestTune$lambda)
Modelolasso$bestTune
coef_lasso<-coef(Modelolasso$finalModel, Modelolasso$bestTune$lambda)
coef_lasso<-coef(Modelolasso$finalModel, Modelolasso$bestTune$lambda)
# Cross-validation
ctrl <- trainControl(
method = "cv",
number = 10) # número de folds
lasso<-train(fmla,
data=hog_training,
method = 'glmnet',
trControl = ctrl,
tuneGrid = expand.grid(alpha = 1, #lasso
lambda = seq(0.001,0.02,by = 0.001)),
preProcess = c("center", "scale")
)
plot(lasso
lambda,
plot(lasso
lambda,
plot(lasso,
lambda,
lasso
RMSE,
plot(lasso,
lambda,
lasso,
RMSE,
xlab="lambda",
ylab="Root Mean-Squared Error (RMSE)"
)
plot(lasso,
lambda,
xlab="lambda",
ylab="Root Mean-Squared Error (RMSE)"
)
plot(lasso
lambda,
plot(lassoresultslambda,
lassoresultsRMSE,
xlab="lambda",
ylab="Root Mean-Squared Error (RMSE)"
)
lasso$bestTune
Modelolasso<-train(fmla,
data=hog_training,
method = 'glmnet',
trControl = ctrl,
tuneGrid = expand.grid(alpha = 1, #lasso
lambda = seq(0.001,0.02,by = 0.001)),
preProcess = c("center", "scale")
)
Modelolasso$bestTune
summary(Modelolasso) # Resumen del modelo
ggplot(varImp(Modelolasso, lambda = grid)) # Gráfico de importancia de las variables
Modelolasso$bestTune
ggplot(varImp(Modelolasso)) # Gráfico de importancia de las variables
Modelolasso$bestTune
## Gráfico de los coeficientes
#Put coefficients in a data frame, except the intercept
coefs_lasso<-data.frame(t(as.matrix(coef(ModeloLasso)))) %>% select(-X.Intercept.)
#add the lambda grid to to data frame
coefs_lasso<- coefs_lasso %>% mutate(lambda=grid)
#ggplot friendly format
coefs_lasso<- coefs_lasso %>% pivot_longer(cols=!lambda,
names_to="variables",
values_to="coefficients")
ggplot(data=coefs_lasso, aes(x = lambda, y = coefficients, color = variables)) +
geom_line() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10",
scales::math_format(10^.x))
) +
labs(title = "Coeficientes Lasso", x = "Lambda", y = "Coeficientes") +
theme_bw() +
theme(legend.position="bottom")
coef_lasso<-coef(Modelolasso$finalModel, Modelolasso$bestTune$lambda)
coef_lasso
plot(lasso$results$lambda,
plot(lasso$results$lambda,
lasso$results$RMSE,
xlab="lambda",
ylab="RMSE")
summary(Modelolasso) # Resumen del modelo
plot(lasso$results$lambda,
lasso$results$RMSE,
xlab="lambda",
ylab="RMSE")
## Predicción 1: Predicciones con hog_testing
pred_test1_Modelolasso <- predict(Modelolasso, newdata = hog_testing) # Predicción
eva_Modelolasso <- data.frame(obs=hog_testing$Ingtotug, pred=pred_test1_Modelolasso) # Data frame con observados y predicciones
metrics_Modelolasso <- metrics(eva_Modelolasso, obs, pred); metrics_Modelolasso # Cálculo del medidas de precisión
# Identificación de pobres y no pobres en hog_testing
pob1_Modelolasso <- ifelse(pred_test1_Modelolasso<hog_testing$Lp, 1, 0)
# Evaluación de clasificación
eva_Modelolasso <- data.frame(obs=as.factor(hog_testing$Pobre), pred=as.factor(pob1_Modelolasso)) # Data frame con observados y predicciones
confmatrix_Modelolasso <- confusionMatrix(data = as.factor(pob1_Modelolasso), reference = as.factor(hog_testing$Pobre)) ; confmatrix_Modelolasso # Matriz de confusión
## Predicción 2: Predicciones con test_hogares
pred_test2_Modelolasso <- predict(Modelolasso, newdata = test_hogares)
# Identificación de pobres y no pobres en test_hogares
pob2_Modelolasso <- ifelse(pred_test2_Modelolasso<test_hogares$Lp, 1, 0)
# Exportar para prueba en Kaggle
Kaggle_Modelolasso <- data.frame(id=test_hogares$id, pobre=pob2_Modelolasso)
write.csv(Kaggle_Modelolasso,"./stores/Kaggle_Modelolasso.csv", row.names = FALSE)
write.csv(Kaggle_Modelolasso,"~/GitHub/Taller-2-BDML/stores/Kaggle_Modelolasso.csv", row.names = FALSE)
metrics_Modelolassokaggle <- metrics(Kaggle_Modelolasso, obs, pred); metrics_Modelolassokaggle # Cálculo del medidas de precisión
plot(ModeloEN$results$lambda,
ModeloEN$results$RMSE,
xlab="lambda",
ylab="RMSE")
### 3.3 Elastic net -----------------------------------------------------------------------------------
ModeloEN<-caret::train(fmla,
data=hog_training,
method = 'glmnet',
trControl = ctrl,
tuneGrid = expand.grid(alpha = seq(0,1,by = 0.01), #Lasso
lambda = seq(0.001,0.1,by = 0.001)),
preProcess = c("center", "scale")
)
plot(ModeloEN$results$lambda,
ModeloEN$results$RMSE,
xlab="lambda",
ylab="RMSE")
overall_accuracy_lasso = cm_Modelolasso$overall['Accuracy']
cm_lasso = confusionMatrix(xtab_Modelolasso)
coef_lasso<-coef(Modelolasso$finalModel, Modelolasso$bestTune$lambda)
coef_lasso
ggpairs(hog_training, columns = 1:4, ggplot2::aes(colour = gender)) +
theme_bw()
variables_categoricas <- names(hog_training[,sapply(df, is.factor)])
for (var in variables_categoricas) {
p2 <- ggplot(hog_training, aes(y = .data[[var]])) +
geom_bar(aes(x = (..count..)/sum(..count..)),
fill = "darkblue") +
labs(title = paste("Distribución de la variable", var),
x = "Proporción (%)") +
scale_x_continuous(labels = scales::percent) +
theme_bw()
}
variables_categoricas <- names(hog_training[,sapply(df, is.factor)])
for (var in variables_categoricas) {
p2 <- ggplot(hog_training, aes(y = .data[[var]])) +
geom_bar(aes(x = (..count..)/sum(..count..)),
fill = "darkblue") +
labs(title = paste("Distribución de la variable", var),
x = "Proporción (%)") +
scale_x_continuous(labels = scales::percent) +
theme_bw()
}
p_load(fastAdaboost)
M_grid<- expand.grid(nIter=c(10,50,100),method="adaboost")
M_grid
set.seed(1011)
adaboost_res <- train(fmla,
data = train,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
adaboost_res <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
adaboost_res <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
p_load(fastAdaboost)
install.packages('BiocManager')
p_install(fastAdaboost, character.only = TRUE, ...)
install.packages('BiocManager')
p_install(fastAdaboost, character.only = TRUE)
p_install(fastAdaboost)
install.packages("fastAdaboost", dependencies = TRUE)
install.packages("C:/Users/lmrod/OneDrive/Escritorio/fastAdaboost_1.0.0.tar.gz", repos = NULL, type = "source")
adaboost_res <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
library(gbm)
install.packages(gbm)
install.packages("gbm")
install.packages("fastAdaboost")
adaboost_res <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
boost <- gbm(fmla, data = hog_training,
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
method = "boosting",
method = "boost",
method = "adaboost",
method = 'adaboost',
boost <- train(fmla,
data = hog_training,
method = 'adaboost',
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE")
boost <- train(fmla,
data = hog_training,
method = 'adaboost',
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE")
install.packages("gbm")
library(gbm)
set.seed (1)
boost <- gbm(fmla, data = hog_training,
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
# Nueva regresión, eliminando las variables que NO fueron seleccionadas por EN
fmla_RF <- formula(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_Desh+prop_contributivo+
prop_media+prop_superior+prop_mayoriatiempotrabajo+prop_obreroemplempresa+prop_obreroemplgobierno+prop_empldomestico+
prop_trabajadorcuentapropia+prop_patronempleador)
ctrl_RF <- trainControl(method = "cv",
number = 10, # Es recomendable correr 10
)
install.packages("fastAdaboost", dependencies = TRUE)
adaboost_res <- train(fmla_RF,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
install.packages("C:/Users/lmrod/OneDrive/Escritorio/fastAdaboost_1.0.0.tar.gz", repos = NULL, type = "source")
adaboost_res <- train(fmla_RF,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
adaboost_res <- train(fmla_RF,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
install.packages("C:/Users/lmrod/OneDrive/Escritorio/fastAdaboost_1.0.0.tar.gz", repos = NULL, type = "source")
adaboost_res <- train(fmla_RF,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
boost <- train(fmla,
data = hog_training,
method = 'adaboost',
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE")
M_grid<- expand.grid(nIter=c(10,50,100),method="adaboost")
M_grid
boost <- train(fmla,
data = hog_training,
method = 'adaboost',
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE")
boost <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE")
adaboost_res <- train(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_Desh+prop_contributivo+
prop_media+prop_superior+prop_mayoriatiempotrabajo+prop_obreroemplempresa+prop_obreroemplgobierno+prop_empldomestico+
prop_trabajadorcuentapropia+prop_patronempleador,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
M_grid<- expand.grid(nIter=c(10,50,100),method="adaboost")
M_grid
set.seed(1011)
adaboost_res <- train(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_Desh+prop_contributivo+
prop_media+prop_superior+prop_mayoriatiempotrabajo+prop_obreroemplempresa+prop_obreroemplgobierno+prop_empldomestico+
prop_trabajadorcuentapropia+prop_patronempleador,
data = hog_training,
method = "adaboost",
trControl = ctrl,
metric = "Accuracy",
tuneGrid = M_grid
)
boost <- train(fmla,
data = hog_training,
method = "adaboost",
trControl = ctrl_RF,
tuneGrid=grid_gbm,
metric = "RMSE")
boost.hog <- gbm(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_Desh+prop_contributivo+
prop_media+prop_superior+prop_mayoriatiempotrabajo+prop_obreroemplempresa+prop_obreroemplgobierno+prop_empldomestico+
prop_trabajadorcuentapropia+prop_patronempleador, data = hog_training,
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
plot(boost.hog , i = "rm")
# 3 lucas
# Creamos una grilla para tunear el gbm
tunegrid_gbm <- expand.grid(learn_rate = c(0.1, 0.01, 0.001),
ntrees = 50,
max_depth = 2,
col_sample_rate = 1,
min_rows = 70)
install.packages("h2o")
if ("package:h2o" %in% search()) {detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) {remove.packages("h2o")}
library(pacman)
p_load("RCurl","jsonlite")
install.packages("h2o", type = "source",
repos = (c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
# Esta chimbada es bien demorada
set.seed(1)
library(caret)
# Inicializamos el modelo
library(h2o)
modelo3 <- train(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_Desh+prop_contributivo+
prop_media+prop_superior+prop_mayoriatiempotrabajo+prop_obreroemplempresa+prop_obreroemplgobierno+prop_empldomestico+
prop_trabajadorcuentapropia+prop_patronempleador,
data = hog_training,
method = "gbm_h2o",
trControl = cv3,
metric = 'RMSE',
tuneGrid = tunegrid_gbm)
ggplot(varImp(Modelolasso)) # Gráfico de importancia de las variables
ggplot(data=coefs_lasso, aes(x = lambda, y = coefficients, color = variables)) +
geom_line() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10",
scales::math_format(10^.x))
) +
labs(title = "Coeficientes Lasso", x = "Lambda", y = "Coeficientes") +
theme_bw() +
theme(legend.position="bottom")
ggplot(varImp(Modelolasso, scale=FALSE)) # Gráfico de importancia de las variables
lassovarimp <- varImp(Modelolasso, scale=FALSE)
lassovarimp
lassovarimp <- varImp(Modelolasso, scale=TRUE)
lassovarimp
confmatrix_Modelolasso
metrics_Modelolasso <- metrics(eva_Modelolasso, obs, pred); metrics_Modelolasso # Cálculo del medidas de precisión
metrics_ModeloRL <- metrics(eva_ModeloRL, obs, pred); metrics_ModeloRL # Cálculo del medidas de precisión
# Fórmula de los modelos
fmla <- formula(Ingtotug~P5000+P5010+P5090+Nper+Npersug+Depto+prop_P6585s1h+prop_P6585s3h+prop_P7510s3h+
prop_P7505h+prop_P6920h+prop_Desh+prop_subsidiado+prop_contributivo+prop_especial+
prop_ningunoeduc+prop_preescolar+prop_basicaprimaria+prop_basicasecundaria+prop_media+prop_superior+
prop_mayoriatiempotrabajo+prop_mayoriatiempobuscandotrabajo+prop_mayoriatiempoestudiando+
prop_mayoriatiempooficiohogar+prop_mayoriatiempoincapacitado+prop_obreroemplempresa+
prop_obreroemplgobierno+prop_empldomestico+prop_trabajadorcuentapropia+prop_patronempleador+
prop_trabajadorsinremunfamilia+prop_trabajadorsinremunempresa)
## Ajuste: Estimación de modelo de regresión lineal con datos de entrenamiento
ModeloRL <- train(fmla,
data = hog_training, method = 'lm',
trControl= ctrl,
preProcess = c("center", "scale"))
# Evaluación de clasificación
eva_ModeloRL <- data.frame(obs=as.factor(hog_testing$Pobre), pred=as.factor(pob1_ModeloRL)) # Data frame con observados y predicciones
confmatrix_ModeloRL <- confusionMatrix(data = as.factor(pob1_ModeloRL), reference = as.factor(hog_testing$Pobre)) ; confmatrix_ModeloRL # Matriz de confusión
metrics_ModeloRL <- metrics(eva_ModeloRL, obs, pred); metrics_ModeloRL # Cálculo del medidas de precisión
# Limpiar el espacio
rm(list = ls(all.names = TRUE))
setwd("C:/Users/lmrod/OneDrive/Documentos/GitHub/Taller-4-BDML")
list.of.packages = c("pacman", "readr","tidyverse", "dplyr", "arsenal", "fastDummies",
"caret", "glmnet", "MLmetrics", "skimr", "plyr", "stargazer",
"ggplot2", "plotly", "corrplot", "Hmisc", "tm", "tidytext",
"wordcloud", "SentimentAnalysis")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(list.of.packages, require, character.only = TRUE)
# Template submission: cuenta de tweeter
precio_bog <- read.csv("./data/submission_template.csv")
# Template submission: cuenta de tweeter
rm(precio_bog)
# Template submission: cuenta de tweeter
tweets <- read.csv("./data/submission_template.csv")
setwd("C:/Users/lmrod/OneDrive/Documentos/GitHub/Taller-4-BDML")
# Template submission: cuenta de tweeter
tweets <- read.csv("./data/submission_template.csv")
# Template submission: cuenta de tweeter
tweets <- read.csv("./data/sample_submission.csv")
head(tweets)
# Train
train <- read_csv("./data/train.csv")
# Train
train <- read_csv("./data/train.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
head(tweets)
head(train)
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
# Test
test <- read_csv("./data/test.csv")
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
text = col_character()
))
head(test)
# Preprocesamiento
tweets_train <- train$text
tweets_train <- removeNumbers(comentarios)
tweets_train <- removePunctuation(comentarios)
tweets_train <- tolower(comentarios)
tweets_train <- stripWhitespace(comentarios)
tweets_train <- removeNumbers(tweets_train)
tweets_train <- removePunctuation(tweets_train)
tweets_train <- tolower(tweets_train)
tweets_train <- stripWhitespace(tweets_train)
print(tweets_train)
tweets_train_tidy <- as.data.frame(comentarios) %>% unnest_tokens( "word", comentarios)
tweets_train_tidy <- as.data.frame(tweets_train) %>% unnest_tokens( "word", tweets_train)
head(tweets_train_tidy)
dim(tweets_train_tidy)
tweets_train_tidy  %>%
count(word, sort = TRUE)   %>%
head()
tweets_train_tidy %>%
count(word) %>%
arrange(desc(n)) %>%
head()
tweets_train_tidy  %>%
count(word, sort = TRUE)   %>%
head()
tweets_train_tidy  %>%
tweets_train_tidy  %>%
count(word, sort = TRUE)   %>%
head()
tweets_train_tidy  %>%
count("word", sort = TRUE)   %>%
head()
tweets_train_tidy  %>%
count("word", sort = TRUE)   %>%
head()
