rm(list = ls(all.names = TRUE))
rm(list = ls(all.names = TRUE))
rm(list = ls(all.names = TRUE))
rm(list = ls(all.names = TRUE))
tweets_train <- train
tweets_train <- removeNumbers(tweets_train)
rm(list = ls(all.names = TRUE))
setwd("/Users/bray/Desktop/Big Data/Talleres/Taller-4-BDML")
list.of.packages = c("pacman", "readr","tidyverse", "dplyr", "tidyr",
"caret", "glmnet", "MLmetrics", "skimr", "stargazer",
"ggplot2", "plotly",  "Hmisc", "tm", "tidytext",
"wordcloud", "SentimentAnalysis", "stopwords", "stringi", "text2vec")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(list.of.packages, require, character.only = TRUE)
cuenta <- read.csv("./data/sample_submission.csv")
# Train
train <- read_csv("./data/train.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
text = col_character()
))
tweets_train <- train
p_load(SnowballC)
p_load(Matrix)
p_load(NLP)
# Eliminamos tildes y caracteres especiales del español
tweets_train <- stri_trans_general(str = train$text, id = "Latin-ASCII")
# Sin URL
tweets_train <- gsub("http\\S+", "", tweets_train)
# Sin tetweets_trainto de retwitteo
tweets_train <- gsub("(rt|via)((?:\\b\\W*@\\w+)+)", "", tweets_train)
# Quitamos @ (@usuario)
tweets_train <- gsub("@\\w+", "", tweets_train)
# Quitamos espacios en blanco extras
tweets_train <- gsub("\\s+", " ", tweets_train)
# Recortar los espacios en blanco iniciales y finales
tweets_train <- gsub("^\\s+|\\s+$", "", tweets_train)
# Quitamos stop words
p_load(stopwords)
# Descargamos la lista de las stopwords en español de dos fuentes diferentes y las combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
####
tweets_train <- removeWords(tweets_train, lista_palabras)
tweets_train <- removeNumbers(tweets_train)
tweets_train <- removePunctuation(tweets_train)
tweets_train <- tolower(tweets_train)
tweets_train <- stripWhitespace(tweets_train)
tweets2 <- as.data.frame(tweets_train) %>% unnest_tokens( "word", tweets_train)
dim(tweets2)
tweets2  %>%
count(word, sort = TRUE)   %>%
head()
tweets2 <- tweets2  %>%
anti_join(tibble(word =stopwords("spanish")))
dim(tweets2)
tweets <-tweets2
stopwords2 <- iconv(stopwords("spanish"), from = "UTF-8", to = "ASCII//TRANSLIT")
tweets <- tweets %>%
anti_join(tibble(word =stopwords2))
dim(tweets)
wordcloud(tweets$word, min.freq = 90,
colors= c(rgb(72/255, 191/255, 169/255),rgb(249/255, 220/255, 92/255), rgb(229/255, 249/255, 147/255)))
wordcloud(tweets$word, min.freq = 90,
colors= c(rgb(72/255, 191/255, 169/255),rgb(249/255, 220/255, 92/255), rgb(229/255, 249/255, 147/255)))
wordcloud(tweets$word, min.freq = 90,
colors= c(rgb(72/255, 191/255, 169/255),rgb(249/255, 220/255, 92/255), rgb(229/255, 249/255, 147/255)))
tweets$radical <- stemDocument(tweets$word, lenguage= "Spanish")
tweets$radical <- stemDocument(tweets$word, lenguage= "spanish")
tweets$radical <- stemDocument( tweets$word, language="spanish")
View(tweets)
bigrams <- as.data.frame(tweets_train) %>%
unnest_tokens(bigram, tweets_train, token = "ngrams", n = 2)
View(bigrams)
stop_words <- data.frame(word1 = stopwords2,
word2 = stopwords2)
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram, sort = TRUE)
ggplot(bigram_freq[1:10, ], aes(y = reorder(bigram, -n), x = n)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ylab("Bigramas") +
xlab("Frecuencia")
p_load(upipe)
p_load(udpipe)
model <- udpipe_download_model(language = "spanish")
model2 <- udpipe_load_model(model$file_model)
bigrams_sep <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
reviews_annotated1 <- udpipe_annotate(model, bigrams_sep$word1)
reviews_annotated1 <- udpipe_annotate(model2, bigrams_sep$word1)
reviews_tibble1 <- as.data.frame(reviews_annotated1) %>%
select(token, upos)
reviews_tibble1 <- as.data.frame(reviews_annotated1) %>%
select(token, upos)
adjetivos1 <- reviews_tibble1 %>%
filter(upos == "ADJ") %>%
select(token)
View(adjetivos1)
p_load(syuzhet)
sentimientos_df <- get_nrc_sentiment(adjetivos1$token, lang="spanish")
adjetivos_unicos <- unique(adjetivos1$token)
sentimientos_df <- get_nrc_sentiment(adjetivos_unicos, lang="spanish")
View(adjetivos1)
library(ggplot2)
ggplot(df_palabras[1:20, ], aes(x = palabra, y = frecuencia, fill = "blue")) +
geom_bar(stat = "identity") +
xlab("Palabras") +
ylab("Frecuencia") +
ggtitle("Frecuencia de palabras en los tweets") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_x_discrete(labels = function(x) str_wrap(x, width = 10))
ggplot(adjetivos1[1:10, ], aes(y = reorder(bigram, -n), x = n)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ylab("Bigramas") +
xlab("Frecuencia")
ggplot(adjetivos1[1:10, ] +
geom_bar(stat = "identity", fill = "#4e79a7") +
ylab("Bigramas") +
xlab("Frecuencia")
ggplot(adjetivos1[1:10, ]) +
ggplot(adjetivos1[1:10, ]) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ylab("Adjetivos") +
xlab("Frecuencia")
adjetivos2<- as.data.frame(adjetivos1)
ggplot(adjetivos2[1:10, ]) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ylab("Adjetivos") +
xlab("Frecuencia")
freq_adjetivos <- table(tweets$adjetivos)
barplot(freq_adjetivos, main="Frecuencia de adjetivos en tweets", xlab="Adjetivos", ylab="Frecuencia")
ggplot(adjetivos1)
ggplot(adjetivos1, aes()) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ggtitle("adjetivos más frecuentes") +
ylab("adjetivos") +
xlab("Frecuencia")
ggplot(adjetivos1, aes(x=adjective, y=frequency)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ggtitle("adjetivos más frecuentes") +
ylab("adjetivos") +
xlab("Frecuencia")
count(adjetivos1)
View(adjetivos1)
# Calcular la frecuencia de cada palabra en la variable token
frecuencias <- table(adjetivos1$token)
# Ordenar la tabla de frecuencias de mayor a menor
frecuencias_ordenadas <- sort(frecuencias, decreasing = TRUE)
# Crear un gráfico de barras que muestre la frecuencia de cada palabra
barplot(frecuencias_ordenadas, main = "Gráfico de frecuencia de palabras", xlab = "Palabras", ylab = "Frecuencia")
# Crear un data frame con las frecuencias de cada palabra
df <- token %>%
filter(base == "adjetivos 1") %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == "adjetivos1") %>%
count(palabra, sort = TRUE)
View(adjetivos1)
df <- token %>%
filter(base == "adjetivos1") %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
View(adjetivos2)
df <- token %>%
filter(base == adjetivos1) %>%
count(palabra, sort = TRUE)
df <- token %>%
filter(base == adjetivos2) %>%
count(palabra, sort = TRUE)
corpus <- Corpus(VectorSource(adjetivos1))
# Preprocesar el texto
corpus <- corpus %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stripWhitespace) %>%
tm_map(tolower)
# Extraer todas las palabras del Corpus
palabras <- unlist(strsplit(as.character(corpus), "\\s+"))
# Contar el número de veces que aparece cada palabra
tabla_palabras <- table(palabras)
# Ordenar la tabla de frecuencia de mayor a menor
tabla_palabras_ordenada <- sort(tabla_palabras, decreasing = TRUE)
# Convertir la tabla en un data frame para graficar
df_palabras <- data.frame(palabra = names(tabla_palabras_ordenada),
frecuencia = as.numeric(tabla_palabras_ordenada))
# Graficar el número de veces que aparece cada palabra
library(ggplot2)
ggplot(df_palabras[1:20, ], aes(x = palabra, y = frecuencia)) +
geom_bar(stat = "identity") +
xlab("Palabras") +
ylab("Frecuencia") +
ggtitle("Frecuencia de palabras en los tweets") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
library(ggplot2)
ggplot(df_palabras[1:20, ], aes(x = palabra, y = frecuencia)) +
geom_bar(stat = "identity") +
xlab("adjetivos") +
ylab("Frecuencia") +
ggtitle("Frecuencia de palabras de los adjetivos") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
library(ggplot2)
ggplot(df_palabras[1:20, ], aes(x = palabra, y = frecuencia, fill = "blue")) +
geom_bar(stat = "identity") +
xlab("adjetivos") +
ylab("Frecuencia") +
ggtitle("Frecuencia de palabras de los adjetivos") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
library(ggplot2)
ggplot(df_palabras[1:20, ], aes(x = palabra, y = frecuencia)) +
geom_bar(stat = "identity") +
xlab("adjetivos") +
ylab("Frecuencia") +
ggtitle("Frecuencia de palabras de los adjetivos") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
View(adjetivos1)
