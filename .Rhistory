# Limpiar el espacio
rm(list = ls(all.names = TRUE))
setwd("C:/Users/lmrod/OneDrive/Documentos/GitHub/Taller-4-BDML")
list.of.packages = c("pacman", "readr","tidyverse", "dplyr", "arsenal", "fastDummies",
"caret", "glmnet", "MLmetrics", "skimr", "plyr", "stargazer",
"ggplot2", "plotly", "corrplot", "Hmisc", "tm", "tidytext",
"wordcloud", "SentimentAnalysis")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(list.of.packages, require, character.only = TRUE)
# Template submission: cuenta de tweeter
cuenta <- read.csv("./data/sample_submission.csv")
# Train
train <- read_csv("./data/train.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
text = col_character()
))
# 2.1.1 ----------------------------  Train -----------------------------------
tweets_train <- train$text # Importamos los datos de training
head(tweets_train_tidy2, n = 30)
# Limpiar el espacio
rm(list = ls(all.names = TRUE))
setwd("C:/Users/lmrod/OneDrive/Documentos/GitHub/Taller-4-BDML")
list.of.packages = c("pacman", "readr","tidyverse", "dplyr", "arsenal", "fastDummies",
"caret", "glmnet", "MLmetrics", "skimr", "plyr", "stargazer",
"ggplot2", "plotly", "corrplot", "Hmisc", "tm", "tidytext",
"wordcloud", "SentimentAnalysis")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(list.of.packages, require, character.only = TRUE)
# Template submission: cuenta de tweeter
cuenta <- read.csv("./data/sample_submission.csv")
# Train
train <- read_csv("./data/train.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
text = col_character()
))
tweets_train <- train$text
#Removemos números, puntuación y espacios en blanco excesivos. Ponemos todo en minúsculas y formato ASCII para mejor manejo.
tweets_train <- removeNumbers(tweets_train)
tweets_train <- removePunctuation(tweets_train)
tweets_train <- tolower(tweets_train)
tweets_train <- stripWhitespace(tweets_train)
tweets_train <- iconv(tweets_train, from = "UTF-8", to = "ASCII//TRANSLIT")
#tokenizamos
tweets_train_tidy <- as.data.frame(tweets_train) %>% unnest_tokens( "word", tweets_train)
dim(tweets_train_tidy)
head(tweets_train_tidy, n = 30)
#Veamos cuáles son las palabras más frecuentes
tweets_train_tidy %>%
dplyr::count(word, sort = TRUE)   %>%
head()
#quitamos stopwords
head(stopwords('spanish'))
tweets_train_tidy2 <- tweets_train_tidy  %>%
anti_join(tibble(word =stopwords("spanish")))
dim(tweets_train_tidy2) # se reducen casi a la mitad la cantidad de palabras
head(tweets_train_tidy2, n = 30)
# vemos cuáles son las palabras más comunes
wordcloud(tweets_train_tidy2$word, min.freq = 100,
colors= c(rgb(72/255, 191/255, 169/255),rgb(249/255, 220/255, 92/255), rgb(229/255, 249/255, 147/255)))
class(tweets_train_tidy2$word)
tweets_train_tidy2$radical <- stemDocument( comentarios_tidy$word, language="spanish")
tweets_train_tidy2$radical <- stemDocument( tweets_train_tidy2$word, language="spanish")
tweets_train_tidy2 %>% head()
tweets_train_tidy2$radical <- stemDocument( tweets_train_tidy2$word, language="spanish")
tweets_train_tidy2 %>% head(n=30)
tweets_train_tidy2$radical <- stemDocument(tweets_train_tidy2$word, language="spanish")
tweets_train_tidy2 %>% head(n=30)
# Generar bigramas a partir del texto de las críticas
bigrams <- as.data.frame(tweets_train) %>%
unnest_tokens(bigram, tweets_train, token = "ngrams", n = 2)
stop_words <- data.frame(word1 = stopwords("es"),
word2 = stopwords("es"))
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram, sort = TRUE)
bigram_freq <- bigrams %>%
count(bigram)
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram, sort = TRUE)
# Visualizar los bigramas más frecuentes
ggplot(bigram_freq[1:10, ], aes(y = reorder(bigram, -n), x = n)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ggtitle("Bigramas más frecuentes") +
ylab("Bigramas") +
xlab("Frecuencia")
bigram_freq <- bigrams %>%
count(bigram, sort = F)
bigram_freq <- bigrams %>%
count(bigram)
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram)
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram)
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigramS)
# Visualizar los bigramas más frecuentes
ggplot(bigram_freq[1:10, ], aes(y = reorder(bigram, -n), x = n)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ggtitle("Bigramas más frecuentes") +
ylab("Bigramas") +
xlab("Frecuencia")
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigrams)
# Visualizar los bigramas más frecuentes
ggplot(bigram_freq[1:10, ], aes(y = reorder(bigram, -n), x = n)) +
geom_bar(stat = "identity", fill = "#4e79a7") +
ggtitle("Bigramas más frecuentes") +
ylab("Bigramas") +
xlab("Frecuencia")
# Limpiar el espacio
rm(list = ls(all.names = TRUE))
setwd("C:/Users/lmrod/OneDrive/Documentos/GitHub/Taller-4-BDML")
list.of.packages = c("pacman", "readr","tidyverse", "dplyr", "arsenal", "fastDummies",
"caret", "glmnet", "MLmetrics", "skimr", "plyr", "stargazer",
"ggplot2", "plotly", "corrplot", "Hmisc", "tm", "tidytext",
"wordcloud", "SentimentAnalysis")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(list.of.packages, require, character.only = TRUE)
# Template submission: cuenta de tweeter
cuenta <- read.csv("./data/sample_submission.csv")
# Train
train <- read_csv("./data/train.csv", col_types = cols(
id = col_character(),
name = col_character(),
text = col_character()
))
# Test
test <- read_csv("./data/test.csv", col_types = cols(
id = col_character(),
text = col_character()
))
tweets_train <- train$text
#Removemos números, puntuación y espacios en blanco excesivos. Ponemos todo en minúsculas y formato ASCII para mejor manejo.
tweets_train <- removeNumbers(tweets_train)
tweets_train <- removePunctuation(tweets_train)
tweets_train <- tolower(tweets_train)
tweets_train <- stripWhitespace(tweets_train)
tweets_train <- iconv(tweets_train, from = "UTF-8", to = "ASCII//TRANSLIT")
#tokenizamos
tweets_train_tidy <- as.data.frame(tweets_train) %>% unnest_tokens( "word", tweets_train)
dim(tweets_train_tidy)
head(tweets_train_tidy, n = 30)
#Veamos cuáles son las palabras más frecuentes
tweets_train_tidy %>%
dplyr::count(word, sort = TRUE)   %>%
head()
#quitamos stopwords
head(stopwords('spanish'))
tweets_train_tidy2 <- tweets_train_tidy  %>%
anti_join(tibble(word =stopwords("spanish")))
dim(tweets_train_tidy2) # se reducen casi a la mitad la cantidad de palabras
head(tweets_train_tidy2, n = 30)
# vemos cuáles son las palabras más comunes
wordcloud(tweets_train_tidy2$word, min.freq = 100,
colors= c(rgb(72/255, 191/255, 169/255),rgb(249/255, 220/255, 92/255), rgb(229/255, 249/255, 147/255)))
class(tweets_train_tidy2$word)
tweets_train_tidy2$radical <- stemDocument(tweets_train_tidy2$word, language="spanish")
tweets_train_tidy2 %>% head(n=30)
# Generar bigramas a partir del texto de las críticas
bigrams <- as.data.frame(tweets_train) %>%
unnest_tokens(bigram, tweets_train, token = "ngrams", n = 2)
stop_words <- data.frame(word1 = stopwords("es"),
word2 = stopwords("es"))
# Eliminar los bigramas que contengan palabras de parada
bigrams <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = "word1") %>%
anti_join(stop_words, by = "word2") %>%
unite(bigram, word1, word2, sep = " ")
bigram_freq <- bigrams %>%
count(bigram)
